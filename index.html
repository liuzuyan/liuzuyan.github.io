<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Zuyan Liu</title>
  
  <meta name="author" content="Zuyan Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zuyan Liu</name>
              </p>
              <p> 
                I am a second-year Ph.D. student at the <a href="http://ivg.au.tsinghua.edu.cn/"> Intelligent Vision Group (IVG)</a>, Department of Automation, Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. Prior to that, I received my Bachelor's degree from the Department of Automation, Tsinghua University in 2023 (Ranking 1/170). 
              </p>
              <p>
                I am broadly inerested in large language model and computer vision. My current research focuses on multi-modal large language models and large vision models.
              </p>
              <p style="text-align:center">
                <a href="mailto:liuzuyan19@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=7npgHqAAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/liuzuyan"> Github </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/lzy.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/ola.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Ola: Pushing the Frontiers of Omni-Modal Language Model</papertitle>
              <br>
              <strong>Zuyan Liu</strong>*, 
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=kMui170AAAAJ"> Yuhao Dong</a>*, 
              Jiahui Wang,
              <a href="https://liuziwei7.github.io/"> Ziwei Liu</a>,
              Winston Hu,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>
              <br>
              <em>arXiv</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2502.04328">[arXiv]</a>
              <a href="https://github.com/Ola-Omni/Ola">[Code]</a>
              <a href="https://ola-omni.github.io/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/N4bjcHOejJudtxTFZVAXmg">[中文解读]</a>
              <a href="https://rank.opencompass.org.cn/leaderboard-multimodal/?m=REALTIME">[Rank 1st on OpenCompass Leaderboard (<15B)]</a>
              <br>
              <p> Ola is an Omni-modal Language model that achieves competitive performance across image, video, and audio understanding compared to specialized models, pushing the frontiers of the omni-modal language model.
			        </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/oryx.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution</papertitle>
              <br>
              <strong>Zuyan Liu</strong>*, 
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=kMui170AAAAJ"> Yuhao Dong</a>*, 
              <a href="https://liuziwei7.github.io/"> Ziwei Liu</a>,
              Winston Hu,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>,
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2409.12961">[arXiv]</a>
              <a href="https://github.com/Oryx-mllm/Oryx">[Code]</a>
              <a href="https://oryx-mllm.github.io/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/IzRYcoAjb7pGwetimPocIg">[中文解读]</a>
              <br>
              <p> Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths.
			        </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:75%;max-width:100%" src="images/insightv.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=kMui170AAAAJ"> Yuhao Dong</a>*, 
              <strong>Zuyan Liu</strong>*, 
              <a href="https://www.lamda.nju.edu.cn/sunhl/"> Hailong Sun</a>, 
              <a href="https://jingkang50.github.io/"> Jingkang Yang</a>, 
              Winston Hu,
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>, 
              <a href="https://liuziwei7.github.io/"> Ziwei Liu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
              <br>
              <font color="red"><strong>Highlight</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2411.14432">[arXiv]</a>
              <a href="https://github.com/dongyh20/Insight-V">[Code]</a>
              <a href="https://mp.weixin.qq.com/s/-8TvvTDa7zeEUlzcbtuPWg">[中文解读]</a>
              <br>
              <p> Insight-V is a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results.
			        </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/elastic.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Efficient Inference of Vision Instruction-Following Models with Elastic Cache</papertitle>
              <br>
              <strong>Zuyan Liu</strong>, 
			        <a href="https://liubl1217.github.io/">Benlin Liu</a>,
              Jiahui Wang,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=kMui170AAAAJ"> Yuhao Dong</a>, 
              <a href="https://chengy12.github.io/"> Guangyi Chen</a>, 
              <a href="https://ranjaykrishna.com/index.html/"> Ranjay Krishna</a>, 
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2407.18121">[arXiv]</a>
              <a href="https://github.com/liuzuyan/ElasticCache">[Code]</a>
              <a href="https://sites.google.com/view/elastic-cache">[Project Page]</a> 
              <br>
              <p> Elastic Cache is a novel approach for KV Cache acceleration in multi-modal large language models that benefits from applying distinct acceleration methods for instruction encoding and output generation stages.
			        </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/chainofspot.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models</papertitle>
              <br>
              <strong>Zuyan Liu</strong>*, 
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=kMui170AAAAJ"> Yuhao Dong</a>*,
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>arXiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.12966">[arXiv]</a>
              <a href="https://github.com/dongyh20/Chain-of-Spot">[Code]</a>
              <a href="https://sites.google.com/view/chain-of-spot/">[Project Page]</a> 
              <br>
              <p> The Chain-of-Spot (CoS) method is an approach that enhances feature extraction by focusing on key regions of interest (ROI) within the image, corresponding to the posed questions or instructions.
			        </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/vpd.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Unleashing Text-to-Image Diffusion Models for Visual Perception</papertitle>
              <br>
              <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>*, 
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>*, 
              <strong>Zuyan Liu</strong>*, 
			        <a href="https://liubl1217.github.io/">Benlin Liu</a>
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.02153">[arXiv]</a>
              <a href="https://github.com/wl-zhao/VPD">[Code]</a>
              <a href="https://vpd.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2?p=unleashing-text-to-image-diffusion-models-for-1">[Rank 1st on NYUv2 Depth Estimation]</a>
              <br>
              <p> VPD (Visual Perception with Pre-trained Diffusion Models) is a framework that leverages the high-level and low-level knowledge of a pre-trained text-to-image diffusion model to downstream visual perception tasks.
			        </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/dynamic.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks</papertitle>
              <br>
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>*, 
              <strong>Zuyan Liu</strong>*, 
              <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>*, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF:24.31)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2207.01580">[arXiv]</a>
              <a href="https://github.com/raoyongming/DynamicViT">[Code]</a>
              <a href="https://dynamicvit.ivg-research.xyz/">[Project Page]</a> 
              <br>
              <p> The dynamic spatial sparsification framework can be applied to general visual architectures (e.g. Transformers, ConvNeXt, Swin Transformers) and visual tasks (e.g. classification, object detection, semantic segmentation) for efficient inference.
			        </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/diffswap.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion</papertitle>
              <br>
              <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>, 
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>*, 
              Weikang Shi, 
              <strong>Zuyan Liu</strong>, 
              <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>*, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_DiffSwap_High-Fidelity_and_Controllable_Face_Swapping_via_3D-Aware_Masked_Diffusion_CVPR_2023_paper.pdf">[arXiv]</a>
              <a href="https://github.com/wl-zhao/DiffSwap">[Code]</a>
              <br>
              <p> DiffSwap is a diffusion model based framework for high-fidelity and controllable face swapping.
			        </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PoinTr.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</papertitle>
              <br>
              <a href="https://yuxumin.github.io/">Xumin Yu</a>*,  
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>*, <a href="https://wangzy22.github.io/"> Ziyi Wang</a>, <strong>Zuyan Liu</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>

              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2108.08839">[arXiv]</a> 
              <a href="https://github.com/yuxumin/PoinTr/">[Code]</a> 
              <a href="https://zhuanlan.zhihu.com/p/401928647">[中文解读]</a>
              <br>
              <p> PoinTr is a transformer-based framework that reformulates point cloud completion as a set-to-set translation problem. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2023 Outstanding Undergraduate, Tsinghua University </li>
                <li style="margin: 5px;"> 2023 Outstanding Undergraduate, Beijing </li>
                <li style="margin: 5px;"> 2022 Chinese National Scholarship </li>
                <li style="margin: 5px;"> 2021 Jiang Nanxiang Scholarship, Tsinghua University </li>
                <li style="margin: 5px;"> 2020 December 9th Scholarship, Tsinghua University </li>
                <li style="margin: 5px;"> 2022,2021,2020 Comprehensive Excellence Scholarship, Tsinghua University </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> NeurIPS 2025, ICLR 2025, ICCV 2025, CVPR 2025, ECCV 2024, CVPR 2024, ICCV 2023
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> Pattern Recognition
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
	  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=_FTR0v3yajZcKlt4vYq6w_2ssjRRhnfkzwEtW_blVyo"></script>
	  </div>        
	  <br>
	    &copy; Zuyan Liu | Last updated: May 25, 2025
</center></p>
</body>

</html>
